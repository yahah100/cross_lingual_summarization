{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: moverscore in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from moverscore) (3.7.4.3)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from moverscore) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyemd in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.24.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.2.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=894090 sha256=4d880ca30bff588d58e5d8867a7cee3123842ecb38231c972081dfc3244c9d69\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-93txtctm/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, click, joblib, tqdm, sacremoses, tokenizers, filelock, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 joblib-1.0.1 regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.10.1 tqdm-4.59.0 transformers-4.3.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install moverscore\n",
    "!pip install torch\n",
    "!pip install pyemd\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the MoverScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b61f31261654f82ba804e1a4097c1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bda6efc6606450993984f4a98ac414d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9d7cee01584129ab5ee22bca00427d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f817b48a6d840938c7283708c574c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8355bd988811440ca9d50a2e3fc65998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference: https://github.com/Yale-LILY/SummEval/blob/master/evaluation/summ_eval/mover_score_metric.py\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import os\n",
    "from pyemd import emd, emd_with_flow\n",
    "from torch import nn\n",
    "from math import log\n",
    "from itertools import chain\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def truncate(tokens):\n",
    "    if len(tokens) > tokenizer.model_max_length - 2:\n",
    "        tokens = tokens[0:(tokenizer.model_max_length - 2)]\n",
    "    return tokens\n",
    "\n",
    "def process(a):\n",
    "    a = [\"[CLS]\"]+truncate(tokenizer.tokenize(a))+[\"[SEP]\"]\n",
    "    a = tokenizer.convert_tokens_to_ids(a)\n",
    "    return set(a)\n",
    "\n",
    "\n",
    "def get_idf_dict(arr, nthreads=4):\n",
    "    idf_count = Counter()\n",
    "    num_docs = len(arr)\n",
    "\n",
    "    process_partial = partial(process)\n",
    "\n",
    "    with Pool(nthreads) as p:\n",
    "        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
    "\n",
    "    idf_dict = defaultdict(lambda : log((num_docs+1)/(1)))\n",
    "    idf_dict.update({idx:log((num_docs+1)/(c+1)) for (idx, c) in idf_count.items()})\n",
    "    return idf_dict\n",
    "\n",
    "def padding(arr, pad_token, dtype=torch.long):\n",
    "    lens = torch.LongTensor([len(a) for a in arr])\n",
    "    max_len = lens.max().item()\n",
    "    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n",
    "    mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n",
    "    for i, a in enumerate(arr):\n",
    "        padded[i, :lens[i]] = torch.tensor(a, dtype=dtype)\n",
    "        mask[i, :lens[i]] = 1\n",
    "    return padded, lens, mask\n",
    "\n",
    "def bert_encode(model, x, attention_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = model(x, attention_mask = attention_mask)\n",
    "    return result.hidden_states\n",
    "\n",
    "#with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "#    stop_words = set(f.read().strip().split(' '))\n",
    "\n",
    "def collate_idf(arr, tokenize, numericalize, idf_dict,\n",
    "                pad=\"[PAD]\"):\n",
    "    \n",
    "    tokens = [[\"[CLS]\"]+truncate(tokenize(a))+[\"[SEP]\"] for a in arr]  \n",
    "    arr = [numericalize(a) for a in tokens]\n",
    "\n",
    "    idf_weights = [[idf_dict[i] for i in a] for a in arr]\n",
    "    \n",
    "    pad_token = numericalize([pad])[0]\n",
    "\n",
    "    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)\n",
    "    padded_idf, _, _ = padding(idf_weights, pad_token, dtype=torch.float)\n",
    "\n",
    "    return padded, padded_idf, lens, mask, tokens\n",
    "\n",
    "def get_bert_embedding(all_sens, model, tokenizer, idf_dict,\n",
    "                       batch_size=-1):\n",
    "\n",
    "    padded_sens, padded_idf, lens, mask, tokens = collate_idf(all_sens,\n",
    "                                                      tokenizer.tokenize, tokenizer.convert_tokens_to_ids,\n",
    "                                                      idf_dict)\n",
    "\n",
    "    if batch_size == -1: batch_size = len(all_sens)\n",
    "\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_sens), batch_size):\n",
    "            batch_embedding = bert_encode(model, padded_sens[i:i+batch_size],\n",
    "                                          attention_mask=mask[i:i+batch_size])\n",
    "            batch_embedding = torch.stack(batch_embedding)\n",
    "            embeddings.append(batch_embedding)\n",
    "            del batch_embedding\n",
    "\n",
    "    total_embedding = torch.cat(embeddings, dim=-3)\n",
    "    return total_embedding, lens, mask, padded_idf, tokens\n",
    "\n",
    "def _safe_divide(numerator, denominator):\n",
    "    return numerator / (denominator + 1e-30)\n",
    "\n",
    "def batched_cdist_l2(x1, x2):\n",
    "    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
    "    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
    "    res = torch.baddbmm(\n",
    "        x2_norm.transpose(-2, -1),\n",
    "        x1,\n",
    "        x2.transpose(-2, -1),\n",
    "        alpha=-2\n",
    "    ).add_(x1_norm).clamp_min_(1e-30).sqrt_()\n",
    "    return res\n",
    "\n",
    "def word_mover_score(refs, hyps, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords = True, batch_size=256):\n",
    "    preds = []\n",
    "    for batch_start in range(0, len(refs), batch_size):\n",
    "        batch_refs = refs[batch_start:batch_start+batch_size]\n",
    "        batch_hyps = hyps[batch_start:batch_start+batch_size]\n",
    "        \n",
    "        ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding(batch_refs, model, tokenizer, idf_dict_ref)\n",
    "        hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding(batch_hyps, model, tokenizer, idf_dict_hyp)\n",
    "\n",
    "        ref_embedding = ref_embedding[-1]\n",
    "        hyp_embedding = hyp_embedding[-1]\n",
    "        \n",
    "        batch_size = len(ref_tokens)\n",
    "        for i in range(batch_size):  \n",
    "            ref_ids = [k for k, w in enumerate(ref_tokens[i]) \n",
    "                                if w in stop_words or '##' in w \n",
    "                                or w in set(string.punctuation)]\n",
    "            hyp_ids = [k for k, w in enumerate(hyp_tokens[i]) \n",
    "                                if w in stop_words or '##' in w\n",
    "                                or w in set(string.punctuation)]\n",
    "          \n",
    "            ref_embedding[i, ref_ids,:] = 0                        \n",
    "            hyp_embedding[i, hyp_ids,:] = 0\n",
    "            \n",
    "            ref_idf[i, ref_ids] = 0\n",
    "            hyp_idf[i, hyp_ids] = 0\n",
    "            \n",
    "        raw = torch.cat([ref_embedding, hyp_embedding], 1)\n",
    "                             \n",
    "        raw.div_(torch.norm(raw, dim=-1).unsqueeze(-1) + 1e-30) \n",
    "        \n",
    "        distance_matrix = batched_cdist_l2(raw, raw).double().cpu().numpy()\n",
    "                \n",
    "        for i in range(batch_size):  \n",
    "            c1 = np.zeros(raw.shape[1], dtype=np.float)\n",
    "            c2 = np.zeros(raw.shape[1], dtype=np.float)\n",
    "            c1[:len(ref_idf[i])] = ref_idf[i]\n",
    "            c2[len(ref_idf[i]):] = hyp_idf[i]\n",
    "            \n",
    "            c1 = _safe_divide(c1, np.sum(c1))\n",
    "            c2 = _safe_divide(c2, np.sum(c2))\n",
    "            \n",
    "            dst = distance_matrix[i]\n",
    "            _, flow = emd_with_flow(c1, c2, dst)\n",
    "            flow = np.array(flow, dtype=np.float32)\n",
    "            score = 1./(1. + np.sum(flow * dst))#1 - np.sum(flow * dst)\n",
    "            preds.append(score)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = [\"Was heute wichtig war - und was Sie auf SZ.de am meisten interessiert hat. Der Tag kompakt Britischer Parlamentspräsident Bercow kündigt Rücktritt an.\", \"this is a test sentence.\", \"What was important today - and what interested you most on SZ.de. In fact, it's not - but its not.\"]\n",
    "references = [\"Was heute wichtig war - und was Sie auf SZ.de am meisten interessiert hat.\", \"I do not like bla\", \"What was important today - and what interested you most on SZ.de. In fact, it's not.\"]\n",
    "\n",
    "# idf_dict_hyp = get_idf_dict(translations) \n",
    "idf_dict_hyp = defaultdict(lambda: 1.)\n",
    "# idf_dict_ref = get_idf_dict(references) \n",
    "idf_dict_ref = defaultdict(lambda: 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = word_mover_score(references, translations, idf_dict_ref, idf_dict_hyp, \\\n",
    "                          stop_words=[], remove_subwords=True)\n",
    "type(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6434998069232901, 0.49437090400373906, 0.8094961853095445]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sens, padded_idf, lens, mask, tokens = collate_idf(translations, tokenizer.tokenize, tokenizer.convert_tokens_to_ids,\n",
    "                                                      idf_dict_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'Was',\n",
       "  'heute',\n",
       "  'wichtig',\n",
       "  'war',\n",
       "  '-',\n",
       "  'und',\n",
       "  'was',\n",
       "  'Sie',\n",
       "  'auf',\n",
       "  'S',\n",
       "  '##Z',\n",
       "  '.',\n",
       "  'de',\n",
       "  'am',\n",
       "  'meisten',\n",
       "  'interes',\n",
       "  '##siert',\n",
       "  'hat',\n",
       "  '.',\n",
       "  'Der',\n",
       "  'Tag',\n",
       "  'kom',\n",
       "  '##pakt',\n",
       "  'Brit',\n",
       "  '##ischer',\n",
       "  'Parlament',\n",
       "  '##sp',\n",
       "  '##r',\n",
       "  '##äsi',\n",
       "  '##dent',\n",
       "  'Be',\n",
       "  '##rco',\n",
       "  '##w',\n",
       "  'k',\n",
       "  '##ün',\n",
       "  '##digt',\n",
       "  'Rücktritt',\n",
       "  'an',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'What',\n",
       "  'was',\n",
       "  'important',\n",
       "  'today',\n",
       "  '-',\n",
       "  'and',\n",
       "  'what',\n",
       "  'interested',\n",
       "  'you',\n",
       "  'most',\n",
       "  'on',\n",
       "  'S',\n",
       "  '##Z',\n",
       "  '.',\n",
       "  'de',\n",
       "  '.',\n",
       "  'In',\n",
       "  'fact',\n",
       "  ',',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'not',\n",
       "  '-',\n",
       "  'but',\n",
       "  'its',\n",
       "  'not',\n",
       "  '.',\n",
       "  '[SEP]']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
