{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "from transformers import ( \n",
    "    T5Tokenizer, \n",
    "    TFT5Model, \n",
    "    TFT5ForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "\n",
    "model_size = \"t5-base\"\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "MAX_ARTICLE_LEN = 512\n",
    "\n",
    "MAX_HIGHLIGHT_LEN = 150\n",
    "\n",
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageTokens:\n",
    "    def __init__(self, tokenizer, tf_or_pt: str) -> None:\n",
    "        super().__init__()\n",
    "        self.en_de_prefix = tokenizer(\"summarize English to German: \", return_tensors=tf_or_pt).input_ids\n",
    "        self.de_en_prefix = tokenizer(\"summarize German to English: \", return_tensors=tf_or_pt).input_ids\n",
    "        self.en_en_prefix = tokenizer(\"summarize English to English: \", return_tensors=tf_or_pt).input_ids\n",
    "        self.de_de_prefix = tokenizer(\"summarize German to German: \", return_tensors=tf_or_pt).input_ids\n",
    "\n",
    "        if tf_or_pt == \"tf\":\n",
    "            self.en_de_prefix = tf.reshape(self.en_de_prefix, (-1,))\n",
    "            self.de_en_prefix = tf.reshape(self.de_en_prefix, (-1,))\n",
    "            self.en_en_prefix = tf.reshape(self.en_en_prefix, (-1,))\n",
    "            self.de_de_prefix = tf.reshape(self.de_de_prefix, (-1,))\n",
    "        elif tf_or_pt == \"pt\":\n",
    "            self.en_de_prefix = self.en_de_prefix.reshape(-1,)\n",
    "            self.de_en_prefix = self.de_en_prefix.reshape(-1,)\n",
    "            self.en_en_prefix = self.en_en_prefix.reshape(-1,)\n",
    "            self.de_de_prefix = self.de_de_prefix.reshape(-1,)\n",
    "\n",
    "        # check if last token is end of sequence token and remove it\n",
    "        if self.en_de_prefix[-1] == 1:\n",
    "            self.en_de_prefix = self.en_de_prefix[:-1]\n",
    "            self.de_en_prefix = self.de_en_prefix[:-1]\n",
    "            self.en_en_prefix = self.en_en_prefix[:-1]\n",
    "            self.de_de_prefix = self.de_de_prefix[:-1]\n",
    "\n",
    "        assert self.en_de_prefix.shape[0] == self.de_en_prefix.shape[0] == self.en_en_prefix.shape[0] == self.de_de_prefix.shape[0], \"All perfixes must have the same size\"\n",
    "        self.prefix_size = self.en_de_prefix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "language_tokens = LanguageTokens(tokenizer, \"tf\")\n",
    "prefix_size = language_tokens.prefix_size\n",
    "en_de_prefix = language_tokens.en_de_prefix\n",
    "de_en_prefix = language_tokens.de_en_prefix\n",
    "en_en_prefix = language_tokens.en_en_prefix\n",
    "de_de_prefix = language_tokens.de_de_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-12.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-7.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-11.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-2.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-6.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-5.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-4.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-10.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-3.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-1.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-14.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-15.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-8.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-0.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-13.tfrecord', '../data/cnn_daily_mail_train//cnn_daily_mail_multilingual-9.tfrecord']\n",
      "['../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-12.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-16.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-7.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-11.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-2.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-6.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-5.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-17.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-4.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-10.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-19.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-3.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-1.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-14.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-15.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-18.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-8.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-0.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-13.tfrecord', '../data/cnn_daily_mail_val//cnn_daily_mail_multilingual-9.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "bucket = \"..\"\n",
    "\n",
    "def get_tf_record_files(directory):\n",
    "    file_list = []\n",
    "    for item in listdir(directory):\n",
    "        if item.split(\".\")[-1] == \"tfrecord\":\n",
    "            file_list.append(\"{}/{}\".format(directory, item))\n",
    "    return file_list\n",
    "\n",
    "def get_tfrecord_dataset(directory):\n",
    "    features = {\n",
    "        'ger_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-prefix_size], tf.int64),\n",
    "        'ger_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-prefix_size], tf.int64),\n",
    "        'ger_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "        'ger_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "\n",
    "        'en_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-prefix_size], tf.int64),\n",
    "        'en_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-prefix_size], tf.int64),\n",
    "        'en_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "        'en_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "    }\n",
    "\n",
    "    tf_records_list = get_tf_record_files(directory)\n",
    "    print(tf_records_list)\n",
    "    dataset = tf.data.TFRecordDataset(tf_records_list)\n",
    "\n",
    "    # TensorFlow models repository: https://github.com/tensorflow/models/blob/befbe0f9fe02d6bc1efb1c462689d069dae23af1/official/nlp/bert/input_pipeline.py#L24\n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        i  = tf.random.uniform((1,),0,4,dtype=tf.int32)[0]\n",
    "        \n",
    "        if i == 0:\n",
    "            return tf.concat([de_de_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(prefix_size, dtype=tf.int32), record['ger_x_mask']], axis=0), record['ger_y'], record['ger_y_ids']\n",
    "        elif i == 1:\n",
    "            return tf.concat([en_de_prefix, record['en_x']], axis=0), tf.concat([tf.ones(prefix_size, dtype=tf.int32), record['en_x_mask']], axis=0), record['ger_y'], record['ger_y_ids']\n",
    "        elif i == 2:\n",
    "            return tf.concat([de_en_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(prefix_size, dtype=tf.int32), record['ger_x_mask']], axis=0), record['en_y'], record['en_y_ids']\n",
    "        else:\n",
    "            return tf.concat([en_en_prefix, record['en_x']], axis=0), tf.concat([tf.ones(prefix_size, dtype=tf.int32), record['en_x_mask']], axis=0), record['en_y'], record['en_y_ids']\n",
    "\n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    dataset = dataset.shuffle(500000)\n",
    "    return dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = get_tfrecord_dataset(bucket + \"/data/cnn_daily_mail_train/\")\n",
    "train_dataset.prefetch(1024)\n",
    "\n",
    "validation_dataset = get_tfrecord_dataset(bucket + \"/data/cnn_daily_mail_val/\")\n",
    "# test_dataset = get_tfrecord_dataset(bucket + \"/data/sueddeutsche_val/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_t5for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared (TFSharedEmbeddings)  multiple                  24674304  \n",
      "_________________________________________________________________\n",
      "encoder (TFT5MainLayer)      multiple                  84954240  \n",
      "_________________________________________________________________\n",
      "decoder (TFT5MainLayer)      multiple                  113275008 \n",
      "=================================================================\n",
      "Total params: 222,903,552\n",
      "Trainable params: 222,903,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
    "@tf.function\n",
    "def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "test_loss_metric = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy')\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_size, output_attentions=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_ids, input_mask, y, y_ids):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, training=True)[0]             \n",
    "        loss = compute_loss(y, logits)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_metric.update_state(loss)\n",
    "    train_accuracy_metric.update_state(y, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_step(input_ids, input_mask, y, y_ids):\n",
    "    logits = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, training=False)[0]  \n",
    "    t_loss = compute_loss(y, logits)\n",
    "\n",
    "    test_loss_metric.update_state(t_loss)\n",
    "    test_accuracy_metric.update_state(y, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4112c7982b65440db53ac16bef5c9e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd9d4b54388>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fd9db97bf20> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd9d4b54388>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fd9db97bf20> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Training step 1000 Accuracy: 0.7289080023765564, Training loss: 218.87454223632812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-784a71c6ccf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtraining_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_accuracy_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "training_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    training_steps = 10\n",
    "    epoch_step = 0\n",
    "    print_every = 1000\n",
    "\n",
    "    ### Training loop ###\n",
    "    for input_ids, input_mask, y, y_ids in tqdm(train_dataset, desc=\"Training\"):\n",
    "        train_step(input_ids, input_mask, y, y_ids)  \n",
    "\n",
    "        train_loss = train_loss_metric.result().numpy().astype(float)\n",
    "        training_loss_list.append(train_loss)\n",
    "        train_accuracy = train_accuracy_metric.result().numpy()\n",
    "\n",
    "        global_step += 1\n",
    "        epoch_step += 1\n",
    "\n",
    "        if epoch_step % print_every == 0:\n",
    "            print(f\"Training step {epoch_step} Accuracy: {train_accuracy}, Training loss: {train_loss}\")\n",
    "\n",
    "\n",
    "    ### Test loop ###\n",
    "    for input_ids, input_mask, y, y_ids in tqdm(val_dataset, desc=\"Evaluating\"):\n",
    "        val_step(input_ids, input_mask, y, y_ids)\n",
    "\n",
    "\n",
    "    ### Output results ###\n",
    "    test_accuracy = test_accuracy_metric.result().numpy()\n",
    "    test_loss = test_loss_metric.result().numpy()\n",
    "    val_loss_list.append(test_loss)\n",
    "    print(f'Epoch: [{epoch}] Validation loss = {test_loss}')\n",
    "\n",
    "    ### Reset metrics ###\n",
    "    test_loss_metric.reset_states()\n",
    "    train_accuracy_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "    test_accuracy_metric.reset_states()\n",
    "    epoch_step = 0\n",
    "\n",
    "    ### savecheckpoint ###\n",
    "    save_checkpoint(\"t5_cnn_daily_mail\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
